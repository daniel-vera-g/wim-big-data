{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Java configuration\n",
    "jarConfigPath = \"/Users/d.veragillard/edu/semester/WIM-1/big-data-advanced-database/bd-project/postgresql-42.7.1.jar\"\n",
    "\n",
    "# Spark configuration\n",
    "allocated_memory = \"5g\"  \n",
    "allocated_cores = \"6\"  \n",
    "\n",
    "# Database configuration\n",
    "database_url = \"jdbc:postgresql://localhost:5432/musicbrainz\"\n",
    "properties = {\"user\": \"musicbrainz\", \"password\": \"musicbrainz\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# --- END OF CONFIGURATION ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "- Initialize Spark session connecting to the Postgres DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:48:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "# We attach more memory to the driver and executors(https://spark.apache.org/docs/latest/tuning.html#memory-management-overview)\n",
    "# We use the G1 garbage collector for better performance(https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning)\n",
    "# We add more cores to the driver and executors(https://spark.apache.org/docs/latest/tuning.html#level-of-parallelism)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MusicBrainz PostgreSQL Connection\") \\\n",
    "    .config(\"spark.jars\", jarConfigPath) \\\n",
    "    .config(\"spark.executor.memory\", allocated_memory) \\\n",
    "    .config(\"spark.driver.memory\", allocated_memory) \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.cores\", allocated_cores) \\\n",
    "    .config(\"spark.driver.cores\", allocated_cores) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "\n",
    "- Get the relevant data from Postgres\n",
    "- Already do cleaning in this stage by only selecting relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get general Artist and Area(The country to predict) and additional Artist/Country information that could hint about the artist country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Read data from artist and area tables with only necessary columns\n",
    "artist_df = spark.read.jdbc(url=database_url, table=\"artist\", properties=properties).select(\"id\", \"name\", \"area\")\n",
    "area_df = spark.read.jdbc(url=database_url, table=\"area\", properties=properties).select(\"id\", \"name\")\n",
    "\n",
    "# Assuming area_df is smaller and can be broadcasted\n",
    "# Broadcast join for artist and area tables\n",
    "artist_country_df = artist_df.join(broadcast(area_df), artist_df.area == area_df.id)\n",
    "\n",
    "# Select relevant columns\n",
    "artist_country_df = artist_country_df.select(artist_df.name, area_df.name.alias(\"country\"))\n",
    "\n",
    "# Read more that could be useful for the analysis\n",
    "language_df = spark.read.jdbc(url=database_url, table=\"language\", properties=properties).select(\"id\", \"name\")\n",
    "alias_df = spark.read.jdbc(url=database_url, table=\"artist_alias\", properties=properties).select(\"artist\", \"name\")\n",
    "\n",
    "# Join tables...\n",
    "# Use explicit column names to avoid ambiguity\n",
    "artist_language_df = artist_df.join(language_df, artist_df.id == language_df.id).select(artist_df.name, language_df.name.alias(\"language\"))\n",
    "artist_alias_df = artist_df.join(alias_df, artist_df.id == alias_df.artist).select(artist_df.name, alias_df.name.alias(\"alias\"))\n",
    "\n",
    "# Combining all data into one dataframe with left outer join\n",
    "combined_artist_df = artist_country_df \\\n",
    "    .join(artist_alias_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_language_df, [\"name\"], \"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data cleaning\n",
    "\n",
    "Handle missing data. F.ex all the NULLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------+\n",
      "|  name|      country|               alias|language|\n",
      "+------+-------------+--------------------+--------+\n",
      "|*NSYNC|United States|              N*SYNC|  Uzekwe|\n",
      "|*NSYNC|United States|             ´N Sync|  Uzekwe|\n",
      "|*NSYNC|United States|              N'Sync|  Uzekwe|\n",
      "|*NSYNC|United States|             'N Sync|  Uzekwe|\n",
      "|*NSYNC|United States|              'NSync|  Uzekwe|\n",
      "|*NSYNC|United States|             N' Sync|  Uzekwe|\n",
      "|*NSYNC|United States|              N-SYNC|  Uzekwe|\n",
      "|*NSYNC|United States|               NSYNC|  Uzekwe|\n",
      "|*NSYNC|United States|             N'Synch|  Uzekwe|\n",
      "|*NSYNC|United States|              N Sync|  Uzekwe|\n",
      "|*NSYNC|United States|              Nsynch|  Uzekwe|\n",
      "|*NSYNC|United States|              ★NSYNC|  Uzekwe|\n",
      "|*NSYNC|United States|              ★NSync|  Uzekwe|\n",
      "|*NSYNC|United States|              *NSYNC|  Uzekwe|\n",
      "|   666|      Germany|         Sei Sei Sei|   Nsari|\n",
      "|   666|      Germany|Triangle Six of D...|   Nsari|\n",
      "|   666|      Germany|666 Triangle Six ...|   Nsari|\n",
      "|   666|   Bangladesh|         Sei Sei Sei|   Nsari|\n",
      "|   666|   Bangladesh|Triangle Six of D...|   Nsari|\n",
      "|   666|   Bangladesh|666 Triangle Six ...|   Nsari|\n",
      "+------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Dropping rows where 'country', 'name' is null or empty\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.country.isNotNull())\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.name.isNotNull())\n",
    "\n",
    "# Remove all rows in combined_artist_df that have null values\n",
    "combined_artist_df = combined_artist_df.na.drop()\n",
    "print(\"Number of rows: \" + str(combined_artist_df.count()))\n",
    "\n",
    "combined_artist_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature transformation\n",
    "\n",
    "Transform feature strings into more suitable formats. To do this:\n",
    "\n",
    "1. Use `StringIndexer` to convert the strings in the columns into indices(Like unique IDs)\n",
    "2. Then use `OneHotEncoder` to convert the categorical indices into a binary vector(F.ex `[0,1,0,...]`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:48:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# String Indexing for all categorical columns\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(combined_artist_df) \n",
    "            for column in [\"name\", \"country\", \"language\", \"alias\"]]\n",
    "\n",
    "# One-Hot Encoding for all indexed columns\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol()+\"_vec\") \n",
    "            for indexer in indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature normalization\n",
    "\n",
    "Scale transformed values to fixed range. To do this:\n",
    "\n",
    "1. Use `VectorAssembler` to combine multiple columns into a single vector column. Helps with machine learning algorithms\n",
    "2. Then apply `StandardScaler`. It helps, to make sure that the model is not influenced by features with larger scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Assembling all the features\n",
    "assemblerInputs = [encoder.getOutputCol() for encoder in encoders]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Feature normalization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally combine all steps into one transformation / normalization pipeline and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:48:39 WARN DAGScheduler: Broadcasting large task binary with size 1559.5 KiB\n",
      "23/12/31 18:48:40 WARN DAGScheduler: Broadcasting large task binary with size 1543.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Building a Pipeline for transformations\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# Transforming the data\n",
    "model = pipeline.fit(combined_artist_df)\n",
    "transformed_df = model.transform(combined_artist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "- Generate test, train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:48:42 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 9823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:48:44 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data Count: 2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:48:47 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Count: 2064\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training, validation, and testing sets\n",
    "train_data, val_data, test_data = transformed_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Show the count of each dataset\n",
    "print(f\"Training Data Count: {train_data.count()}\")\n",
    "print(f\"Validation Data Count: {val_data.count()}\")\n",
    "print(f\"Testing Data Count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Training\n",
    "\n",
    "- Select model\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:48:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/31 18:48:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/12/31 18:48:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:48:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:03 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:03 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:06 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:06 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:11 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:11 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:12 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: DenseMatrix([[-5.99243545e-01,  4.24459041e-01,  1.33329163e-01, ...,\n",
      "              -1.49719556e-02, -2.32857031e-02,  0.00000000e+00],\n",
      "             [ 3.66389123e-01,  3.05987535e-01,  1.51954467e-01, ...,\n",
      "               1.89706244e-03, -9.75152186e-03,  0.00000000e+00],\n",
      "             [ 3.35403633e-01, -1.15056554e-01,  1.27548917e-01, ...,\n",
      "               4.58226118e-04, -1.32741295e-03,  0.00000000e+00],\n",
      "             ...,\n",
      "             [ 1.66738882e-04, -1.67219635e-04, -5.79973378e-04, ...,\n",
      "               1.43369402e-05,  1.48928215e-05,  0.00000000e+00],\n",
      "             [-1.66760880e-04, -7.70518054e-04, -1.37538846e-03, ...,\n",
      "               1.14108275e-05,  1.24972812e-05,  0.00000000e+00],\n",
      "             [ 8.39534774e-04, -8.63281495e-04, -2.91724274e-03, ...,\n",
      "               7.27501149e-05,  7.57907907e-05,  0.00000000e+00]])\n",
      "Intercept: [7.960963422339186,6.707949359498985,4.949541056970745,3.58626118846711,3.6417932808680957,3.634700081158457,3.4849159877524314,3.3063714835456626,3.3228193214692037,3.330130217509457,3.3635792927604644,3.1617118451139166,2.7428925571370155,2.7108525960097367,2.6622798333303863,2.638740290973668,2.4907633449450537,2.478326819425136,2.3585863363999224,2.2092774040338052,2.328503721104493,2.072516702493064,2.252511895659503,2.246656271709436,2.134607842460736,2.0387233117931784,2.0230100794146284,1.7293459040332169,1.8690406304190348,1.8131030251729252,1.7045137985433212,1.5322202203963344,1.6867584325018126,1.613120389225652,1.4282357095689402,1.5906191145462538,1.4069672099104875,1.5943732734936493,1.3987537065743154,1.3465965054986693,1.2853646858698724,1.2866029524573162,1.312354269988069,1.1831013384951896,1.2501607153805536,1.2516313862804032,1.1357758342748343,1.0724533837574384,0.9202612909101127,1.1699940678240468,0.9932478874950802,0.8463567722457525,1.0467351579865802,0.9934839350845055,1.0631933044025277,0.7574930123244791,0.9903304489222654,1.0607177925799975,1.134676008561245,0.8427005194456791,0.84592521007251,0.8387900143686514,0.4314650338131847,0.5599914686180382,0.5553838900354556,0.668040272429517,0.8292839517754464,0.6707805026247952,0.7544187072820716,0.5586719278041109,0.3118243990292086,0.4307995549485229,0.6559543473582572,0.3117409257243933,0.3014433125776073,0.5506433100948797,0.4348613038113812,0.6584509999393976,0.44436105512261814,0.650071967287135,-0.03515478758654505,0.561287765449234,0.44338185060442753,0.43428008110875194,-0.25668937852055224,0.3002193620479649,0.5595912626822039,0.5524497385580743,-0.5406663980223534,0.31259222845761586,-0.2512103046005687,0.15514397149925907,0.4381512234444632,0.15376965744496002,0.2981202973538333,0.44638683187205497,0.30353970755399257,0.31113517034402993,0.44483799199335433,-0.025531410384770237,0.4425471155102285,0.14788535307831757,0.5642253619510347,0.15697718305044636,0.15325242810253742,0.30312022836323593,0.15784046293283102,0.30706828901708566,0.1571051460027724,0.1484980829071992,-0.02778849449805999,0.15422095092349156,0.301726621473463,0.3038630578921804,-0.03515478757743745,-0.027227547278518973,-0.25502445717601857,0.1455643332742771,0.15453037909281228,-0.03214471342707449,-0.24990914527611402,0.14916472006695425,-0.24975202282683087,0.15576271655223753,0.15315663628968948,-0.25668937851860163,-0.03252332713923249,0.15370321897317943,-0.03374977074533402,0.15363728398119206,-0.026252044237385957,0.15396277247741225,0.1497634907070816,-0.2522831278610459,-0.029939251836425585,-0.03515478756860583,-0.03515478756137964,0.14556433326817617,0.1527045849612571,0.15760104195571698,-0.026568667694075735,-0.2508987982442131,-0.03515478756396996,0.14881983771134638,-0.2532394510141707,-0.028975960542429895,-0.5427593562386112,-0.2502028110538538,-0.2503208851798653,-0.035153677519241186,-0.03515478756488235,-0.2504610131734498,-0.256689378515259,-0.2510960642284643,-0.2505951068531837,-0.2550780936440551,-0.5389658778613011,-0.2505951068511806,-0.02758718512381904,-0.5427593562376377,-0.5397141748632013,-0.5427593562377562,-0.02936943020562046,-0.029336654714337423,-0.2526502602413847,-0.5380884805588196,-0.9466211816749751,-0.027467514572909593,-0.2516159065829713,-0.9444379787131559,-0.02763487379562852,-0.541414541273463,-0.5380159729034533,-0.24978918926913493,-0.9449046472359098,-0.25668937850745155,-0.5384608809299181,-0.9466211816755278,-0.24978918926790022,-0.5402149023319613,-0.25668937850404355,-0.9466211816743663,-0.5410353628270581,-0.5387835064936485,-0.9447237669961346,-0.2558493245613357,-0.25308009683916227,-0.5427593562376634,-0.5422566622718678,-0.2517890124157262,-0.2534026773588395,-0.5379016011352128,-0.5404695451235124,-0.5421865089274026,-0.5388771896791649,-0.251786204347015,-0.5427593562391584,-0.9466211816735363,-0.25261092531031376,-0.5378912209743166,-0.2566886005449173,-0.9446946929872162,-0.2520783023234943,-0.9442892684751792,-0.5383012476316311,-0.9445887742217448,-0.5421383274614404,-0.5386893432485245,-0.9445654466274309,-0.5412426404362839,-0.9466211816721745,-0.9445689431029632,-0.9466211816713282,-0.5410235774924473,-1.6382211113547114,-0.9442980909240051,-0.9463390764327672,-0.541669878428629,-0.9447257405228615,-0.5389042651765313,-0.9466211816701953,-0.9466211816701633,-0.5408770123265891,-0.5402231833299572,-0.5402231833282538,-0.9458835259769154,-0.9466211816692829,-0.5389806728834056,-0.538568440139664,-0.5385360340995583,-0.5427589364737706,-0.9442980909216592,-0.5400655908428629,-0.5401410665115114,-0.944903717176166,-0.5427589364744088,-0.541031801739756,-0.5427593562360286,-0.5410827808529817,-0.5387575498928897,-0.9458835259823494,-0.946621181670371,-1.6382211113547114,-0.9457704665877856,-1.6382211113547114,-1.6382211113547114,-0.945237311340744,-0.9443949729762712,-1.6382211113547114,-0.9466211816678108,-1.6382211113547114,-0.9466211816677609,-1.6382211113547114,-0.9458835259796717,-0.9458835259797598,-1.6382211113547114,-1.6382211113547114,-1.6382211113547114,-1.6382211113547114,-0.9443704061984127,-0.9444644368066217,-1.6382211113547114,-0.9455859694095633,-1.6382211113547114,-0.946621181666805,-0.9444644368054503,-0.9458835259776429,-0.9466211816675957,-0.9466211816675755,-0.9459066568060729,-1.6382211113547114,-0.9466211816663743,-0.9466211816656028,-0.9443704061961737,-1.6382211113547114,-0.9443704061961685,-0.9466211816655484,-0.945311702129318,-0.9452219938969072,-0.9447902429936677,-0.9464458906876027,-0.9458835259763764,-1.6382211113547114,-0.9459523340555954,-0.9464647699099676,-0.9454176549496462,-1.6382211113547114,-0.945237311339556,-0.946621181665377,-0.9443925498531318,-0.9458835259764639,-0.9466211816642442,-0.9456875101904306,-1.6382211113547114,-0.94662118166466,-1.6382211113547114,-0.9443351432489792,-0.9453555196883995,-0.9445163781833049,-1.6382211113547114,-1.6382211113547114,-0.9466211816634632,-0.9460072416545456,-0.9466211816622173,-0.9453555196875265,-1.6382211113547114,-0.9446083080432638,-1.6382211113547114,-1.6382211113547114,-1.6382211113547114,-0.9452946322081842,-1.6382211113547114,-0.9464400523729147,-0.9466211816600822,-0.9444602213855059,-0.9466211816589168,-1.6382211113547114,-0.944394972984042,-1.6382211113547114,-1.6382211113547114,-0.9447902429975819,-0.012547154085994319]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:49:23 WARN DAGScheduler: Broadcasting large task binary with size 34.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "False Positive Rate:  0.0\n",
      "True Positive Rate:  1.0000000000000058\n",
      "F-Measure:  1.0000000000000058\n",
      "Precision:  1.0000000000000058\n",
      "Recall:  1.0000000000000058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"country_index\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "# You can also print a summary of the model over the training set\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"Accuracy: \", trainingSummary.accuracy)\n",
    "print(\"False Positive Rate: \", trainingSummary.weightedFalsePositiveRate)\n",
    "print(\"True Positive Rate: \", trainingSummary.weightedTruePositiveRate)\n",
    "print(\"F-Measure: \", trainingSummary.weightedFMeasure())\n",
    "print(\"Precision: \", trainingSummary.weightedPrecision)\n",
    "print(\"Recall: \", trainingSummary.weightedRecall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- Validate model performance\n",
    "- Adjust parameters respectively (Hyperparameter Tuning, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:49:25 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:30 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:30 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:30 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:38 WARN DAGScheduler: Broadcasting large task binary with size 32.7 MiB\n",
      "23/12/31 18:49:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:40 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:40 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:40 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:48 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:49 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:49 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:49 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:50 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:50 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:50 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:49:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:03 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:03 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:06 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:06 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:06 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:11 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:11 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:12 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:12 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:12 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:19 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:19 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:23 WARN DAGScheduler: Broadcasting large task binary with size 32.7 MiB\n",
      "23/12/31 18:50:24 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:24 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:24 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:24 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:25 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:25 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:30 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:30 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:35 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:35 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:38 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:40 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:48 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:49 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:50 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:50 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:54 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:50:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:03 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:03 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:04 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:06 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:10 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:11 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:12 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:12 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:14 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:16 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:19 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:19 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:22 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:22 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:23 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:23 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:24 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:24 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:25 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:25 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:27 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:28 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:30 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:30 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:32 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:35 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:35 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:38 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:40 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:40 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:48 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:48 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:49 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:49 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:50 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:50 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/12/31 18:51:53 ERROR Executor: Exception in task 8.0 in stage 1540.0 (TID 3227)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge(DifferentiableLossAggregator.scala:56)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge$(DifferentiableLossAggregator.scala:49)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.merge(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$2(RDDLossFunction.scala:60)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda/0x000000030242f290.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$foldByKey$3(PairRDDFunctions.scala:219)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x0000000302264988.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1$adapted(ExternalSorter.scala:194)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda/0x0000000302283e78.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:200)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x0000000301d007c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "23/12/31 18:51:53 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#91,Executor task launch worker for task 8.0 in stage 1540.0 (TID 3227),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge(DifferentiableLossAggregator.scala:56)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge$(DifferentiableLossAggregator.scala:49)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.merge(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$2(RDDLossFunction.scala:60)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda/0x000000030242f290.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$foldByKey$3(PairRDDFunctions.scala:219)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x0000000302264988.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1$adapted(ExternalSorter.scala:194)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda/0x0000000302283e78.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:200)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x0000000301d007c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "23/12/31 18:51:53 WARN TaskSetManager: Lost task 8.0 in stage 1540.0 (TID 3227) (l7drwy1994.fritz.box executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge(DifferentiableLossAggregator.scala:56)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge$(DifferentiableLossAggregator.scala:49)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.merge(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$2(RDDLossFunction.scala:60)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda/0x000000030242f290.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$foldByKey$3(PairRDDFunctions.scala:219)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x0000000302264988.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1$adapted(ExternalSorter.scala:194)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda/0x0000000302283e78.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:200)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x0000000301d007c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\n",
      "23/12/31 18:51:53 ERROR TaskSetManager: Task 8 in stage 1540.0 failed 1 times; aborting job\n",
      "23/12/31 18:51:53 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1540.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1540.0 (TID 3227) (l7drwy1994.fritz.box executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge(DifferentiableLossAggregator.scala:56)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge$(DifferentiableLossAggregator.scala:49)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.merge(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$2(RDDLossFunction.scala:60)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda/0x000000030242f290.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$foldByKey$3(PairRDDFunctions.scala:219)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x0000000302264988.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1$adapted(ExternalSorter.scala:194)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda/0x0000000302283e78.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:200)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x0000000301d007c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1199)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1193)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1286)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1253)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1239)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1239)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:52)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:31)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.phi$1(StrongWolfe.scala:76)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$7(StrongWolfe.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:151)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:82)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:38)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n",
      "\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:79)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1015)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:634)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge(DifferentiableLossAggregator.scala:56)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.merge$(DifferentiableLossAggregator.scala:49)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.merge(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$2(RDDLossFunction.scala:60)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda/0x000000030242f290.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$foldByKey$3(PairRDDFunctions.scala:219)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x0000000302264988.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1(ExternalSorter.scala:195)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$insertAll$1$adapted(ExternalSorter.scala:194)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda/0x0000000302283e78.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n",
      "\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:200)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x0000000301d007c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[213.472s][warning][gc,alloc] Executor task launch worker for task 8.0 in stage 1540.0 (TID 3227): Retried waiting for GCLocker too often allocating 4233014 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 18:51:53 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:595)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:547)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3861)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3859)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:499)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "23/12/31 18:51:53 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:595)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:547)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3861)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3859)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:499)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/d.veragillard/edu/semester/WIM-1/big-data-advanced-database/bd-project/wim-big-data/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/vd/11sllxms2s5_n4960jhr3rs40000gn/T/ipykernel_90985/1531727782.py\", line 17, in <module>\n",
      "    cvModel = cv.fit(train_data)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "                                                             ^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run cross-validation, and choose the best set of parameters.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Use the best model to make predictions on the validation data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/edu/semester/WIM-1/big-data-advanced-database/bd-project/wim-big-data/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/edu/semester/WIM-1/big-data-advanced-database/bd-project/wim-big-data/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a ParamGrid for tuning parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Create a CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(labelCol=\"country_index\", predictionCol=\"prediction\"), \n",
    "                    numFolds=3)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "# Use the best model to make predictions on the validation data\n",
    "val_predictions = cvModel.transform(val_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"country_index\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(val_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(val_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(f\"Validation F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- On unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model to make predictions on the test data\n",
    "test_predictions = cvModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = evaluator.evaluate(test_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "test_f1 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment\n",
    "\n",
    "- Deploy into Cloud?\n",
    "\n",
    "#### Monitoring / Maintainance\n",
    "\n",
    "- ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
