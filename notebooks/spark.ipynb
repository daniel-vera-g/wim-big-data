{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "- Initialize Spark session connecting to the Postgres DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "jarConfigPath = \"\"\n",
    "\n",
    "# Initialize Spark session\n",
    "# We attach more memory to the driver and executors(https://spark.apache.org/docs/latest/tuning.html#memory-management-overview)\n",
    "# We use the G1 garbage collector for better performance(https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MusicBrainz PostgreSQL Connection\") \\\n",
    "    .config(\"spark.jars\", jarConfigPath) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "database_url = \"jdbc:postgresql://localhost:5432/musicbrainz\"\n",
    "properties = {\"user\": \"musicbrainz\", \"password\": \"musicbrainz\", \"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "\n",
    "- Get the relevant data from Postgres\n",
    "- Already do cleaning in this stage by only selecting relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get general Artist and Area(The origin to predict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from artist and area tables\n",
    "artist_df = spark.read.jdbc(url=database_url, table=\"artist\", properties=properties).select(\"id\", \"name\", \"area\")\n",
    "area_df = spark.read.jdbc(url=database_url, table=\"area\", properties=properties).select(\"id\", \"name\", \"type\")\n",
    "\n",
    "# Join artist and area tables for detailed country information\n",
    "artist_country_df = artist_df.join(area_df, artist_df.area == area_df.id)\n",
    "\n",
    "# Select relevant columns\n",
    "artist_country_df = artist_country_df.select(artist_df.name, area_df.name.alias(\"country\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add additional Artist/Country information that could hint about the artist country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading additional tables that could have useful information\n",
    "alias_df = spark.read.jdbc(url=database_url, table=\"artist_alias\", properties=properties).select(\"artist\", \"name\")\n",
    "artist_credit_df = spark.read.jdbc(url=database_url, table=\"artist_credit\", properties=properties).select(\"id\", \"name\")\n",
    "language_df = spark.read.jdbc(url=database_url, table=\"language\", properties=properties).select(\"id\", \"name\")\n",
    "script_df = spark.read.jdbc(url=database_url, table=\"script\", properties=properties).select(\"id\", \"name\")\n",
    "\n",
    "# Joining tables\n",
    "\n",
    "# Joining artist with alias\n",
    "# Don't use \"name\" as join column, because it's ambiguous. Use \"alias\"\n",
    "# Same for the other joins\n",
    "artist_alias_df = artist_df.join(alias_df, artist_df.id == alias_df.artist).select(artist_df.name, alias_df.name.alias(\"alias\"))\n",
    "artist_credit_joined_df = artist_df.join(artist_credit_df, artist_df.id == artist_credit_df.id).select(artist_df.name, artist_credit_df.name.alias(\"credit\"))\n",
    "artist_language_df = artist_df.join(language_df, artist_df.id == language_df.id).select(artist_df.name, language_df.name.alias(\"language\"))\n",
    "artist_script_df = artist_df.join(script_df, artist_df.id == script_df.id).select(artist_df.name, script_df.name.alias(\"script\"))\n",
    "\n",
    "# Combining all data into one dataframe\n",
    "combined_artist_df = artist_country_df \\\n",
    "    .join(artist_alias_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_credit_joined_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_language_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_script_df, [\"name\"], \"left_outer\")\n",
    "\n",
    "# Show a sample of data\n",
    "# combined_artist_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data cleaning\n",
    "\n",
    "Handle missing data. F.ex all the NULLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "# TODO - Replacing with empty values causes errors with OneHotEncoder\n",
    "# Replace NULL in all features as it could have an empty value with\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"alias\", when(col(\"alias\").isNull(), \"\").otherwise(col(\"alias\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"credit\", when(col(\"credit\").isNull(), \"\").otherwise(col(\"credit\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"script\", when(col(\"script\").isNull(), \"\").otherwise(col(\"script\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"language\", when(col(\"language\").isNull(), \"\").otherwise(col(\"language\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"country\", when(col(\"country\").isNull(), \"\").otherwise(col(\"country\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"name\", when(col(\"name\").isNull(), \"\").otherwise(col(\"name\")))\n",
    "\n",
    "# Replacing Null values with None as an empty string errors in OneHotEncoder\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"alias\", when(col(\"alias\").isNull(), None).otherwise(col(\"alias\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"credit\", when(col(\"credit\").isNull(), None).otherwise(col(\"credit\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"script\", when(col(\"script\").isNull(), None).otherwise(col(\"script\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"language\", when(col(\"language\").isNull(), None).otherwise(col(\"language\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"country\", when(col(\"country\").isNull(), None).otherwise(col(\"country\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"name\", when(col(\"name\").isNull(), None).otherwise(col(\"name\")))\n",
    "\n",
    "# Dropping rows where 'country', 'name', alias, credit, script or language is null or empty\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.country.isNotNull())\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.name.isNotNull())\n",
    "\n",
    "# TODO do something more useful with these rows\n",
    "# Should work with the Cleaning pipeline\n",
    "# Drop all rows where alias, credit, script or language is NULL\n",
    "combined_artist_df = combined_artist_df.filter(col(\"alias\").isNotNull())\n",
    "combined_artist_df = combined_artist_df.filter(col(\"credit\").isNotNull())\n",
    "combined_artist_df = combined_artist_df.filter(col(\"script\").isNotNull())\n",
    "combined_artist_df = combined_artist_df.filter(col(\"language\").isNotNull())\n",
    "\n",
    "# Show a sample of data\n",
    "combined_artist_df.show()\n",
    "# Count the number of rows\n",
    "print(\"Number of rows: \" + str(combined_artist_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature transformation\n",
    "\n",
    "Transform feature strings into more suitable formats. To do this:\n",
    "\n",
    "1. Use `StringIndexer` to convert the strings in the columns into indices(Like unique IDs)\n",
    "2. Then use `OneHotEncoder` to convert the categorical indices into a binary vector(F.ex `[0,1,0,...]`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# String Indexing for all categorical columns\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(combined_artist_df) \n",
    "            for column in [\"name\", \"country\", \"alias\", \"credit\", \"language\", \"script\"]]\n",
    "\n",
    "# One-Hot Encoding for all indexed columns\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol()+\"_vec\") \n",
    "            for indexer in indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature normalization\n",
    "\n",
    "Scale transformed values to fixed range. To do this:\n",
    "\n",
    "1. Use `VectorAssembler` to combine multiple columns into a single vector column. Helps with machine learning algorithms\n",
    "2. Then apply `StandardScaler`. It helps, to make sure that the model is not influenced by features with larger scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Assembling all the features\n",
    "assemblerInputs = [encoder.getOutputCol() for encoder in encoders]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Feature normalization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally combine all steps into one transformation / cleaning pipeline and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Pipeline for transformations\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# TODO - Empty sting values (in alias) cause errors with OneHotEncoder\n",
    "# Print which column is indexed\n",
    "# print(\"StringIndexer Input Column: \", indexers[2].getInputCol(), \"Output Column: \", indexers[2].getOutputCol())\n",
    "# pipeline = Pipeline(stages=[indexers[2], encoders[2]])\n",
    "\n",
    "# Transforming the data\n",
    "model = pipeline.fit(combined_artist_df)\n",
    "transformed_df = model.transform(combined_artist_df)\n",
    "\n",
    "# Showing the processed DataFrame\n",
    "transformed_df.select(\"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "- Generate test, train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation, and testing sets\n",
    "train_data, val_data, test_data = transformed_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Show the count of each dataset\n",
    "print(f\"Training Data Count: {train_data.count()}\")\n",
    "print(f\"Validation Data Count: {val_data.count()}\")\n",
    "print(f\"Testing Data Count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Training\n",
    "\n",
    "- Select model\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"country_index\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "# You can also print a summary of the model over the training set\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"Accuracy: \", trainingSummary.accuracy)\n",
    "print(\"False Positive Rate: \", trainingSummary.weightedFalsePositiveRate)\n",
    "print(\"True Positive Rate: \", trainingSummary.weightedTruePositiveRate)\n",
    "print(\"F-Measure: \", trainingSummary.weightedFMeasure())\n",
    "print(\"Precision: \", trainingSummary.weightedPrecision)\n",
    "print(\"Recall: \", trainingSummary.weightedRecall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- Validate model performance\n",
    "- Adjust parameters respectively (Hyperparameter Tuning, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a ParamGrid for tuning parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Create a CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(labelCol=\"country_index\", predictionCol=\"prediction\"), \n",
    "                    numFolds=3)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "# Use the best model to make predictions on the validation data\n",
    "val_predictions = cvModel.transform(val_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"country_index\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(val_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(val_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(f\"Validation F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- On unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model to make predictions on the test data\n",
    "test_predictions = cvModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = evaluator.evaluate(test_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "test_f1 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment\n",
    "\n",
    "- Deploy into Cloud?\n",
    "\n",
    "#### Monitoring / Maintainance\n",
    "\n",
    "- ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
