{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Java configuration\n",
    "jarConfigPath = \"\"\n",
    "\n",
    "# Spark configuration\n",
    "allocated_memory = \"6g\"  \n",
    "allocated_cores = \"6\"  \n",
    "\n",
    "# Database configuration\n",
    "database_url = \"jdbc:postgresql://localhost:5432/musicbrainz\"\n",
    "properties = {\"user\": \"musicbrainz\", \"password\": \"musicbrainz\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# --- END OF CONFIGURATION ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "- Initialize Spark session connecting to the Postgres DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "# We attach more memory to the driver and executors(https://spark.apache.org/docs/latest/tuning.html#memory-management-overview)\n",
    "# We use the G1 garbage collector for better performance(https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning)\n",
    "# We add more cores to the driver and executors(https://spark.apache.org/docs/latest/tuning.html#level-of-parallelism)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MusicBrainz PostgreSQL Connection\") \\\n",
    "    .config(\"spark.jars\", jarConfigPath) \\\n",
    "    .config(\"spark.executor.memory\", allocated_memory) \\\n",
    "    .config(\"spark.driver.memory\", allocated_memory) \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.cores\", allocated_cores) \\\n",
    "    .config(\"spark.driver.cores\", allocated_cores) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "\n",
    "- Get the relevant data from Postgres\n",
    "- Already do cleaning in this stage by only selecting relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get general Artist and Area(The country to predict) and additional Artist/Country information that could hint about the artist country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Read data from artist and area tables with only necessary columns\n",
    "artist_df = spark.read.jdbc(url=database_url, table=\"artist\", properties=properties).select(\"id\", \"name\", \"area\")\n",
    "area_df = spark.read.jdbc(url=database_url, table=\"area\", properties=properties).select(\"id\", \"name\")\n",
    "\n",
    "# Assuming area_df is smaller and can be broadcasted\n",
    "# Broadcast join for artist and area tables\n",
    "artist_country_df = artist_df.join(broadcast(area_df), artist_df.area == area_df.id)\n",
    "\n",
    "# Select relevant columns\n",
    "artist_country_df = artist_country_df.select(artist_df.name, area_df.name.alias(\"country\"))\n",
    "\n",
    "# Read more that could be useful for the analysis\n",
    "language_df = spark.read.jdbc(url=database_url, table=\"language\", properties=properties).select(\"id\", \"name\")\n",
    "alias_df = spark.read.jdbc(url=database_url, table=\"artist_alias\", properties=properties).select(\"artist\", \"name\")\n",
    "\n",
    "# Join tables...\n",
    "# Use explicit column names to avoid ambiguity\n",
    "artist_language_df = artist_df.join(language_df, artist_df.id == language_df.id).select(artist_df.name, language_df.name.alias(\"language\"))\n",
    "artist_alias_df = artist_df.join(alias_df, artist_df.id == alias_df.artist).select(artist_df.name, alias_df.name.alias(\"alias\"))\n",
    "\n",
    "# Combining all data into one dataframe with left outer join\n",
    "combined_artist_df = artist_country_df \\\n",
    "    .join(artist_alias_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_language_df, [\"name\"], \"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data cleaning\n",
    "\n",
    "Handle missing data. F.ex all the NULLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Dropping rows where 'country', 'name' is null or empty\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.country.isNotNull())\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.name.isNotNull())\n",
    "\n",
    "# Remove all rows in combined_artist_df that have null values\n",
    "combined_artist_df = combined_artist_df.na.drop()\n",
    "\n",
    "combined_artist_df = combined_artist_df.limit(10000)\n",
    "combined_artist_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature transformation\n",
    "\n",
    "Transform feature strings into more suitable formats. To do this:\n",
    "\n",
    "1. Use `StringIndexer` to convert the strings in the columns into indices(Like unique IDs)\n",
    "2. Then use `OneHotEncoder` to convert the categorical indices into a binary vector(F.ex `[0,1,0,...]`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# String Indexing for all categorical columns\n",
    "# Features:\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\").fit(combined_artist_df) \n",
    "            for column in [\"name\", \"language\", \"alias\"]]  # Exclude 'country'\n",
    "# Label:\n",
    "label_stringIdx = StringIndexer(inputCol=\"country\", outputCol=\"country_index\")\n",
    "\n",
    "# One-Hot Encoding for all indexed columns\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol()+\"_vec\") \n",
    "            for indexer in indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature normalization\n",
    "\n",
    "Scale transformed values to fixed range. To do this:\n",
    "\n",
    "1. Use `VectorAssembler` to combine multiple columns into a single vector column. Helps with machine learning algorithms\n",
    "2. Then apply `StandardScaler`. It helps, to make sure that the model is not influenced by features with larger scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Assembling all the features\n",
    "assemblerInputs = [encoder.getOutputCol() for encoder in encoders]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Feature normalization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally combine all steps into one transformation / normalization pipeline and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Pipeline for transformations\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, label_stringIdx])\n",
    "\n",
    "# Transforming the data\n",
    "model = pipeline.fit(combined_artist_df)\n",
    "transformed_df = model.transform(combined_artist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "- Generate test, train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation, and testing sets\n",
    "train_data, val_data, test_data = transformed_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Show the count of each dataset\n",
    "print(f\"Training Data Count: {train_data.count()}\")\n",
    "print(f\"Validation Data Count: {val_data.count()}\")\n",
    "print(f\"Testing Data Count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Training\n",
    "\n",
    "- Select model\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"country_index\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"Accuracy: \", trainingSummary.accuracy)\n",
    "print(\"False Positive Rate: \", trainingSummary.weightedFalsePositiveRate)\n",
    "print(\"True Positive Rate: \", trainingSummary.weightedTruePositiveRate)\n",
    "print(\"F-Measure: \", trainingSummary.weightedFMeasure())\n",
    "print(\"Precision: \", trainingSummary.weightedPrecision)\n",
    "print(\"Recall: \", trainingSummary.weightedRecall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- Validate model performance\n",
    "- Adjust parameters respectively (Hyperparameter Tuning, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a ParamGrid for tuning parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Create a CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(labelCol=\"country_index\", predictionCol=\"prediction\"), \n",
    "                    numFolds=3)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "# Use the best model to make predictions on the validation data\n",
    "val_predictions = cvModel.transform(val_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"country_index\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(val_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(val_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(f\"Validation F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- On unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model to make predictions on the test data\n",
    "test_predictions = cvModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = evaluator.evaluate(test_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "test_f1 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
