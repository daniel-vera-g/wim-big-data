{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "- Initialize Spark session connecting to the Postgres DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MusicBrainz PostgreSQL Connection\") \\\n",
    "    .config(\"spark.jars\", \"/Users/d.veragillard/edu/semester/WIM-1/big-data-advanced-database/bd-project/postgresql-42.7.1.jar\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "database_url = \"jdbc:postgresql://localhost:5432/musicbrainz\"\n",
    "properties = {\"user\": \"musicbrainz\", \"password\": \"musicbrainz\", \"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "\n",
    "- Get the relevant data from Postgres\n",
    "- Already do cleaning in this stage by only selecting relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get general Artist and Area(The origin to predict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from artist and area tables\n",
    "artist_df = spark.read.jdbc(url=database_url, table=\"artist\", properties=properties).select(\"id\", \"name\", \"area\")\n",
    "area_df = spark.read.jdbc(url=database_url, table=\"area\", properties=properties).select(\"id\", \"name\", \"type\")\n",
    "\n",
    "# Join artist and area tables for detailed country information\n",
    "artist_country_df = artist_df.join(area_df, artist_df.area == area_df.id)\n",
    "\n",
    "# Select relevant columns\n",
    "artist_country_df = artist_country_df.select(artist_df.name, area_df.name.alias(\"country\"))\n",
    "\n",
    "# Show a sample of data\n",
    "# artist_country_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add additional Artist/Country information that could hint about the artist country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading additional tables that could have useful information\n",
    "alias_df = spark.read.jdbc(url=database_url, table=\"artist_alias\", properties=properties).select(\"artist\", \"name\")\n",
    "artist_credit_df = spark.read.jdbc(url=database_url, table=\"artist_credit\", properties=properties).select(\"id\", \"name\")\n",
    "language_df = spark.read.jdbc(url=database_url, table=\"language\", properties=properties).select(\"id\", \"name\")\n",
    "script_df = spark.read.jdbc(url=database_url, table=\"script\", properties=properties).select(\"id\", \"name\")\n",
    "\n",
    "# Joining tables\n",
    "\n",
    "# Joining artist with alias\n",
    "# Don't use \"name\" as join column, because it's ambiguous. Use \"alias\"\n",
    "# Same for the other joins\n",
    "artist_alias_df = artist_df.join(alias_df, artist_df.id == alias_df.artist).select(artist_df.name, alias_df.name.alias(\"alias\"))\n",
    "artist_credit_joined_df = artist_df.join(artist_credit_df, artist_df.id == artist_credit_df.id).select(artist_df.name, artist_credit_df.name.alias(\"credit\"))\n",
    "artist_language_df = artist_df.join(language_df, artist_df.id == language_df.id).select(artist_df.name, language_df.name.alias(\"language\"))\n",
    "artist_script_df = artist_df.join(script_df, artist_df.id == script_df.id).select(artist_df.name, script_df.name.alias(\"script\"))\n",
    "\n",
    "# Combining all data into one dataframe\n",
    "combined_artist_df = artist_country_df \\\n",
    "    .join(artist_alias_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_credit_joined_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_language_df, [\"name\"], \"left_outer\") \\\n",
    "    .join(artist_script_df, [\"name\"], \"left_outer\")\n",
    "\n",
    "# Show a sample of data\n",
    "# combined_artist_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data cleaning\n",
    "\n",
    "Handle missing data. F.ex all the NULLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "# TODO - Replacing with empty values causes errors with OneHotEncoder\n",
    "# Replace NULL in all features as it could have an empty value with\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"alias\", when(col(\"alias\").isNull(), \"\").otherwise(col(\"alias\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"credit\", when(col(\"credit\").isNull(), \"\").otherwise(col(\"credit\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"script\", when(col(\"script\").isNull(), \"\").otherwise(col(\"script\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"language\", when(col(\"language\").isNull(), \"\").otherwise(col(\"language\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"country\", when(col(\"country\").isNull(), \"\").otherwise(col(\"country\")))\n",
    "# combined_artist_df = combined_artist_df.withColumn(\"name\", when(col(\"name\").isNull(), \"\").otherwise(col(\"name\")))\n",
    "\n",
    "# Replacing Null values with None as an empty string errors in OneHotEncoder\n",
    "combined_artist_df = combined_artist_df.withColumn(\"alias\", when(col(\"alias\").isNull(), None).otherwise(col(\"alias\")))\n",
    "combined_artist_df = combined_artist_df.withColumn(\"credit\", when(col(\"credit\").isNull(), None).otherwise(col(\"credit\")))\n",
    "combined_artist_df = combined_artist_df.withColumn(\"script\", when(col(\"script\").isNull(), None).otherwise(col(\"script\")))\n",
    "combined_artist_df = combined_artist_df.withColumn(\"language\", when(col(\"language\").isNull(), None).otherwise(col(\"language\")))\n",
    "combined_artist_df = combined_artist_df.withColumn(\"country\", when(col(\"country\").isNull(), None).otherwise(col(\"country\")))\n",
    "combined_artist_df = combined_artist_df.withColumn(\"name\", when(col(\"name\").isNull(), None).otherwise(col(\"name\")))\n",
    "\n",
    "# Dropping rows where 'country' or 'name' is null or empty\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.country.isNotNull())\n",
    "combined_artist_df = combined_artist_df.filter(combined_artist_df.name.isNotNull())\n",
    "\n",
    "# Show a sample of data\n",
    "# combined_artist_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature transformation\n",
    "\n",
    "Transform feature strings into more suitable formats. To do this:\n",
    "\n",
    "1. Use `StringIndexer` to convert the strings in the columns into indices(Like unique IDs)\n",
    "2. Then use `OneHotEncoder` to convert the categorical indices into a binary vector(F.ex `[0,1,0,...]`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# String Indexing for all categorical columns\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(combined_artist_df) \n",
    "            for column in [\"name\", \"country\", \"alias\", \"credit\", \"language\", \"script\"]]\n",
    "\n",
    "# One-Hot Encoding for all indexed columns\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol()+\"_vec\") \n",
    "            for indexer in indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature normalization\n",
    "\n",
    "Scale transformed values to fixed range. To do this:\n",
    "\n",
    "1. Use `VectorAssembler` to combine multiple columns into a single vector column. Helps with machine learning algorithms\n",
    "2. Then apply `StandardScaler`. It helps, to make sure that the model is not influenced by features with larger scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Assembling all the features\n",
    "assemblerInputs = [encoder.getOutputCol() for encoder in encoders]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Feature normalization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally combine all steps into one transformation / cleaning pipeline and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Pipeline for transformations\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# TODO - Empty sting values (in alias) cause errors with OneHotEncoder\n",
    "# Print which column is indexed\n",
    "# print(\"StringIndexer Input Column: \", indexers[2].getInputCol(), \"Output Column: \", indexers[2].getOutputCol())\n",
    "# pipeline = Pipeline(stages=[indexers[2], encoders[2]])\n",
    "\n",
    "# Transforming the data\n",
    "model = pipeline.fit(combined_artist_df)\n",
    "transformed_df = model.transform(combined_artist_df)\n",
    "\n",
    "# Showing the processed DataFrame\n",
    "transformed_df.select(\"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "- Generate test, train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Training\n",
    "\n",
    "- Select model\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- Validate model performance\n",
    "- Adjust parameters respectively (Hyperparameter Tuning, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- On unseen Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment\n",
    "\n",
    "- Deploy into Cloud?\n",
    "\n",
    "#### Monitoring / Maintainance\n",
    "\n",
    "- ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
